{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def curl_command(cmd):\n",
        "    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    print(f\"Status Code: {process.returncode}\")\n",
        "    print(process.stdout if process.stdout else process.stderr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SPARK_VERSION set to: 3.5.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['SPARK_VERSION'] = '3.5.5'  # Set Spark version\n",
        "print(f\"SPARK_VERSION set to: {os.environ['SPARK_VERSION']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hive distro at /Users/pnilangekar/apache-hive-3.1.3-bin\n",
            "Found existing iceberg runtime JAR\n",
            "Vefitying Spark conf...\n",
            "# HIVE_METASTORE_ICEBERG_TESTCONF\n",
            "Hive metastore iceberg conf already set\n",
            "Killing any running metastore server...\n",
            "Deleting metastore database and warehouse...\n",
            "Initializing metastore schema...\n",
            "Metastore connection URL:\t jdbc:derby:;databaseName=/tmp/data/hms/metastore_db;;create=true\n",
            "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
            "Metastore connection User:\t APP\n",
            "Starting metastore schema initialization to 3.1.0\n",
            "Initialization script hive-schema-3.1.0.derby.sql\n",
            "Initialization script completed\n",
            "schemaTool completed\n",
            "Starting metastore server...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run_shell_command(command):\n",
        "    try:\n",
        "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        \n",
        "        # Print output in real-time\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "                \n",
        "        # Get the return code\n",
        "        return_code = process.poll()\n",
        "        \n",
        "        # Print any errors if the command failed\n",
        "        if return_code != 0:\n",
        "            print(\"Error output:\", file=sys.stderr)\n",
        "            print(process.stderr.read(), file=sys.stderr)\n",
        "            raise subprocess.CalledProcessError(return_code, command)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error executing command: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "os.environ['SPARK_VERSION'] = '3.5.5'  # Set Spark version\n",
        "# Execute the setup_hms.sh script with clean_metastore argument\n",
        "run_shell_command(\"./setup_hms.sh clean_metastore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Session created successfully with Spark version: 3.5.5\n",
            "Spark Configuration: [('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.port', '51907'), ('spark.submit.pyFiles', '/Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.driver.host', '127.0.0.1'), ('spark.app.startTime', '1753469836230'), ('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.repl.local.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.files', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1753469836281'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.app.initial.jar.urls', 'spark://127.0.0.1:51907/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.app.submitTime', '1753469380957'), ('spark.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.sql.catalog.spark_catalog.warehouse', '/tmp/data/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.initial.file.urls', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.app.name', 'Hive Federation Demo'), ('spark.master', 'local[*]'), ('spark.sql.catalog.spark_catalog.uri', 'thrift://localhost:9083'), ('spark.submit.deployMode', 'client'), ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.catalog.spark_catalog.type', 'hive')]\n",
            "\n",
            "Executing: CREATE NAMESPACE IF NOT EXISTS ns1\n",
            "Success!\n",
            "\n",
            "Executing: CREATE NAMESPACE IF NOT EXISTS ns2\n",
            "Success!\n",
            "\n",
            "Executing: DROP TABLE IF EXISTS ns1.table1\n",
            "Success!\n",
            "\n",
            "Executing: DROP TABLE IF EXISTS ns2.table2\n",
            "Success!\n",
            "\n",
            "Executing: CREATE TABLE ns1.table1 (key STRING, value STRING, version INT)\n",
            "Success!\n",
            "\n",
            "Executing: CREATE TABLE ns2.table2 (key STRING, value STRING, version INT)\n",
            "Success!\n",
            "\n",
            "--- Inserting Data ---\n",
            "Data inserted successfully.\n",
            "\n",
            "--- Verifying Tables ---\n",
            "Listing all tables in ns1:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns1|   table1|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Listing all tables in ns2:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns2|   table2|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "--- Reading Data ---\n",
            "Data from ns1.table1:\n",
            "+-------+------------+-------+\n",
            "|    key|       value|version|\n",
            "+-------+------------+-------+\n",
            "| Engine|       Spark|      1|\n",
            "| Engine|Apache Spark|      2|\n",
            "|Catalog|        Hive|      1|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Data from ns2.table2:\n",
            "+----------+------------+-------+\n",
            "|       key|       value|version|\n",
            "+----------+------------+-------+\n",
            "|    Engine|   Snowflake|      1|\n",
            "|Alt Engine|Apache Spark|      1|\n",
            "|   Catalog|Open Catalog|      1|\n",
            "+----------+------------+-------+\n",
            "\n",
            "\n",
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "iceberg_version = \"1.5.2\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# --- Spark Session Builder ---\n",
        "# This configuration is cleaner and follows best practices.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive Federation Demo\") \\\n",
        "    .config(\"spark.jars.packages\", iceberg_package) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/data/spark-warehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Verification ---\n",
        "# Print the Spark version and some key configurations to verify.\n",
        "print(f\"Spark Session created successfully with Spark version: {spark.version}\")\n",
        "conf = spark.sparkContext.getConf()\n",
        "print(f\"Spark Configuration: {conf.getAll()}\")\n",
        "\n",
        "\n",
        "# --- Example Usage: Create Namespaces and Tables ---\n",
        "# The key fix is adding \"USING iceberg\" to your CREATE TABLE statements.\n",
        "# Create namespaces and tables\n",
        "commands = [\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns1\",\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns2\",\n",
        "    \"DROP TABLE IF EXISTS ns1.table1\",\n",
        "    \"DROP TABLE IF EXISTS ns2.table2\",\n",
        "    \"CREATE TABLE ns1.table1 (key STRING, value STRING, version INT)\",\n",
        "    \"CREATE TABLE ns2.table2 (key STRING, value STRING, version INT)\"\n",
        "]\n",
        "\n",
        "# Execute each command and print the result\n",
        "for cmd in commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    try:\n",
        "        result = spark.sql(cmd)\n",
        "        if cmd.strip().upper().startswith((\"SHOW\", \"SELECT\", \"DESCRIBE\")):\n",
        "            result.show()\n",
        "        else:\n",
        "            print(\"Success!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Insert some data\n",
        "print(\"\\n--- Inserting Data ---\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns1.table1 VALUES ('Engine', 'Spark', 1), ('Engine', 'Apache Spark', 2), ('Catalog', 'Hive', 1)\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns2.table2 VALUES ('Engine', 'Snowflake', 1), ('Alt Engine', 'Apache Spark', 1), ('Catalog', 'Open Catalog', 1)\")\n",
        "print(\"Data inserted successfully.\")\n",
        "\n",
        "# Verify the tables were created\n",
        "print(\"\\n--- Verifying Tables ---\")\n",
        "print(\"Listing all tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns1\").show()\n",
        "print(\"\\nListing all tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns2\").show()\n",
        "\n",
        "# Verify data can be read\n",
        "print(\"\\n--- Reading Data ---\")\n",
        "print(\"Data from ns1.table1:\")\n",
        "spark.table(\"spark_catalog.ns1.table1\").show()\n",
        "print(\"\\nData from ns2.table2:\")\n",
        "spark.table(\"spark_catalog.ns2.table2\").show()\n",
        "\n",
        "\n",
        "# Stop the session when done\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up POLARIS auth token. \n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "POLARIS_HOST = os.getenv('POLARIS_HOST', 'localhost')\n",
        "BASE_URL = f\"http://{POLARIS_HOST}:8181\"\n",
        "\n",
        "def get_polaris_token():\n",
        "    response = requests.post(f\"{BASE_URL}/api/catalog/v1/oauth/tokens\",\n",
        "                             data={'grant_type': 'client_credentials', 'scope': 'PRINCIPAL_ROLE:ALL'},\n",
        "                             auth=('root', 's3cr3t'))\n",
        "    return response.json()['access_token']\n",
        "\n",
        "POLARIS_TOKEN = get_polaris_token()\n",
        "\n",
        "os.environ['POLARIS_TOKEN'] = POLARIS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def curl_command(cmd):\n",
        "    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    print(f\"Status Code: {process.returncode}\")\n",
        "    print(process.stdout if process.stdout else process.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   473    0     0  100   473      0   4280 --:--:-- --:--:-- --:--:--  4300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the JSON payload\n",
        "json_data = '''{\n",
        "  \"type\": \"EXTERNAL\",\n",
        "  \"name\": \"federated_hive\",\n",
        "  \"connectionConfigInfo\": {\n",
        "    \"connectionType\": \"HIVE\",\n",
        "    \"uri\": \"thrift://127.0.0.1:9083\",\n",
        "    \"warehouse\": \"hms\",\n",
        "    \"authenticationParameters\": {\n",
        "      \"authenticationType\": \"IMPLICIT\"\n",
        "    }\n",
        "  },\n",
        "  \"properties\": {\n",
        "    \"default-base-location\": \"file:///tmp/data/spark-warehouse\"\n",
        "  },\n",
        "  \"storageConfigInfo\": {\n",
        "    \"storageType\": \"FILE\",\n",
        "    \"allowedLocations\": [\n",
        "      \"file:///tmp/data/spark-warehouse\"\n",
        "    ]\n",
        "  }\n",
        "}'''\n",
        "\n",
        "# Construct and execute the curl command\n",
        "cmd = f'''curl -X POST http://polaris:8181/api/management/v1/catalogs \\\n",
        "    --resolve polaris:8181:127.0.0.1 \\\n",
        "    -H \"Accepts: application/json\" \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $POLARIS_TOKEN\" \\\n",
        "    -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 67\n",
            "\n",
            "{\"namespaces\":[[\"default\"],[\"ns1\"],[\"ns2\"]],\"next-page-token\":null}\n"
          ]
        }
      ],
      "source": [
        "# List all namespaces. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 78\n",
            "\n",
            "{\"identifiers\":[{\"namespace\":[\"ns1\"],\"name\":\"table1\"}],\"next-page-token\":null}\n"
          ]
        }
      ],
      "source": [
        "# List all tables in the ns1 namespace. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 1820\n",
            "ETag: W/\"01b37ffb5ed151f418b5f27d7b1355828a5303c8bc1ad3485fc202d039258220\"\n",
            "\n",
            "{\"metadata-location\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/00001-3b1dade4-4a6d-4003-8fd4-907f2c304cac.metadata.json\",\"metadata\":{\"format-version\":2,\"table-uuid\":\"612a00ca-a120-4dd3-a0e7-f8e10dab3bcb\",\"location\":\"file:/tmp/data/spark-warehouse/ns1.db/table1\",\"last-sequence-number\":1,\"last-updated-ms\":1753469836912,\"last-column-id\":3,\"current-schema-id\":0,\"schemas\":[{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"key\",\"required\":false,\"type\":\"string\"},{\"id\":2,\"name\":\"value\",\"required\":false,\"type\":\"string\"},{\"id\":3,\"name\":\"version\",\"required\":false,\"type\":\"int\"}]}],\"default-spec-id\":0,\"partition-specs\":[{\"spec-id\":0,\"fields\":[]}],\"last-partition-id\":999,\"default-sort-order-id\":0,\"sort-orders\":[{\"order-id\":0,\"fields\":[]}],\"properties\":{\"owner\":\"pnilangekar\",\"write.parquet.compression-codec\":\"zstd\"},\"current-snapshot-id\":8294883887551015881,\"refs\":{\"main\":{\"snapshot-id\":8294883887551015881,\"type\":\"branch\"}},\"snapshots\":[{\"sequence-number\":1,\"snapshot-id\":8294883887551015881,\"timestamp-ms\":1753469836912,\"summary\":{\"operation\":\"append\",\"spark.app.id\":\"local-1753469836281\",\"added-data-files\":\"3\",\"added-records\":\"3\",\"added-files-size\":\"2698\",\"changed-partition-count\":\"1\",\"total-records\":\"3\",\"total-files-size\":\"2698\",\"total-data-files\":\"3\",\"total-delete-files\":\"0\",\"total-position-deletes\":\"0\",\"total-equality-deletes\":\"0\"},\"manifest-list\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/snap-8294883887551015881-1-598171cb-5cab-44b0-99ea-15346a2cd158.avro\",\"schema-id\":0}],\"statistics\":[],\"partition-statistics\":[],\"snapshot-log\":[{\"timestamp-ms\":1753469836912,\"snapshot-id\":8294883887551015881}],\"metadata-log\":[{\"timestamp-ms\":1753469836630,\"metadata-file\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/00000-5804becd-9436-4509-a929-d7607987da60.metadata.json\"}]}}\n"
          ]
        }
      ],
      "source": [
        "# List table metadata for ns1.table1. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/table1 \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 201 Created\n",
            "content-encoding: identity\n",
            "content-length: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Grant TABLE_WRITE_DATA to the catalog_admin role. \n",
        "json_data = '''{\"type\": \"catalog\", \"privilege\": \"TABLE_WRITE_DATA\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/catalogs/federated_hive/catalog-roles/catalog_admin/grants \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 201 Created\n",
            "content-encoding: identity\n",
            "content-length: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assign the catalog_admin role to the service_admin.\n",
        "json_data = '''{\"name\": \"catalog_admin\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/principal-roles/service_admin/catalog-roles/federated_hive \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "{\"connectionConfigInfo\":{\"warehouse\":\"hms\",\"connectionType\":\"HIVE\",\"uri\":\"thrift://127.0.0.1:9083\",\"authenticationParameters\":{\"authenticationType\":\"IMPLICIT\"}},\"type\":\"EXTERNAL\",\"name\":\"federated_hive\",\"properties\":{\"default-base-location\":\"file:///tmp/data/spark-warehouse\"},\"createTimestamp\":1753472204833,\"lastUpdateTimestamp\":1753472204833,\"entityVersion\":1,\"storageConfigInfo\":{\"storageType\":\"FILE\",\"allowedLocations\":[\"file:///tmp/data/spark-warehouse\"]}}\n"
          ]
        }
      ],
      "source": [
        "cmd = f'''curl -X GET -H \"Authorization: Bearer $POLARIS_TOKEN\" -H 'Accept: application/json' -H 'Content-Type: application/json' \\\n",
        "    http://localhost:8181/api/management/v1/catalogs/federated_hive/'''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 3.5.5\n",
            "Active Spark Configuration:\n",
            "[('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.app.initial.jar.urls', 'spark://127.0.0.1:53867/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.repl.local.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.app.id', 'local-1753471474023'), ('spark.driver.port', '53867'), ('spark.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.app.initial.file.urls', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.sql.catalog.spark_catalog.uri', 'thrift://localhost:9083'), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'Polaris Hive Federation'), ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.submit.pyFiles', '/Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.driver.host', '127.0.0.1'), ('spark.files', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar'), ('spark.sql.defaultCatalog', 'polaris'), ('spark.executor.id', 'driver'), ('spark.sql.catalog.polaris.token', 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwb2xhcmlzIiwic3ViIjoiMSIsImlhdCI6MTc1MzQ3MDIyMSwiZXhwIjoxNzUzNDczODIxLCJqdGkiOiJiZjA5NTI1My1kOGEyLTQyYmEtODg5Yy0wYzI0YWJhZWU4MDAiLCJhY3RpdmUiOnRydWUsImNsaWVudF9pZCI6InJvb3QiLCJwcmluY2lwYWxJZCI6MSwic2NvcGUiOiJQUklOQ0lQQUxfUk9MRTpBTEwifQ.EjsjFEtlbgZL1Angtmr4u0ltx5PW2jNA2ofLuMjJKcBnB_VIC8xItas6rEHduUzuqdqDhO7jLTTlU3UzqTTwhiD_Mz2zTlD3mo--1h2iapA9EO8gm3_gHDEBMO09wxBu3ORCHoKj41_b7AV6-tBwoIf5Ld3zeSVDBnArhhYXNKWfgmj45teI7fLARKqCXLKZnnEO-K2MFQi4fHtvWkiTt6plAlr3-GMOuCZsj192QHVjmdC1WoXfyOkyoMFd7nRihz3exbcpQgr0jFqXXFRDOAptYmzP8NGPQ1mT7VU-_h3ERIWaYDnqMb0NFze8-JcWILbg9zcRukqaFbSQ4ovhzw'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.warehouse.dir', 'file:/Users/pnilangekar/polaris/regtests/spark-warehouse'), ('spark.sql.catalog.polaris.warehouse', 'federated_hive'), ('spark.app.startTime', '1753471473833'), ('spark.app.submitTime', '1753469380957'), ('spark.sql.catalog.spark_catalog.warehouse', '/tmp/data/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.catalog.spark_catalog.type', 'hive')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/07/25 12:36:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "iceberg_version = \"1.9.0\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# Create Spark session with the same configurations as spark-sql command\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Polaris Hive Federation\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.jars.packages\",  \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.iceberg:iceberg-aws-bundle:1.7.1,software.amazon.awssdk:bundle:2.30.25,net.snowflake:snowflake-jdbc:3.13.32,com.azure:azure-storage-file-datalake:12.17.0,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.5\") \\\n",
        "     .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.token\", os.environ.get('POLARIS_TOKEN')) \\\n",
        "    .config(\"spark.sql.catalog.polaris.type\", \"rest\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.uri\", \"http://localhost:8181/api/catalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.warehouse\", \"federated_hive\") \\\n",
        "    .config(\"spark.sql.defaultCatalog\", \"polaris\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.client.region\", \"us-east-1\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", \"vended-credentials\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "# Verify configuration\n",
        "print(\"Active Spark Configuration:\")\n",
        "print(spark.sparkContext.getConf().getAll())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing all namespaces:\n",
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|      ns1|\n",
            "|      ns2|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Listing tables in ns1:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns1|   table1|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Listing tables in ns2:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns2|   table2|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Executing: INSERT INTO ns1.table1 (key, value, version) \n",
            "       VALUES \n",
            "       ('key1', 'value1', 1),\n",
            "       ('key2', 'value2', 2),\n",
            "       ('key3', 'value3', 3)\n",
            "Success!\n",
            "\n",
            "Executing: INSERT INTO ns2.table2 (key, value, version)\n",
            "       VALUES\n",
            "       ('keyA', 'valueA', 10),\n",
            "       ('keyB', 'valueB', 20),\n",
            "       ('keyC', 'valueC', 30)\n",
            "Success!\n",
            "\n",
            "Data in ns1.table1:\n",
            "+-------+------------+-------+\n",
            "|    key|       value|version|\n",
            "+-------+------------+-------+\n",
            "|   key1|      value1|      1|\n",
            "|   key2|      value2|      2|\n",
            "|   key3|      value3|      3|\n",
            "| Engine|       Spark|      1|\n",
            "| Engine|Apache Spark|      2|\n",
            "|Catalog|        Hive|      1|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Data in ns2.table2:\n",
            "+----------+------------+-------+\n",
            "|       key|       value|version|\n",
            "+----------+------------+-------+\n",
            "|    Engine|   Snowflake|      1|\n",
            "|      keyA|      valueA|     10|\n",
            "|Alt Engine|Apache Spark|      1|\n",
            "|      keyB|      valueB|     20|\n",
            "|   Catalog|Open Catalog|      1|\n",
            "|      keyC|      valueC|     30|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List all namespaces\n",
        "print(\"Listing all namespaces:\")\n",
        "spark.sql(\"SHOW NAMESPACES\").show()\n",
        "\n",
        "# List tables in ns1 and ns2\n",
        "print(\"\\nListing tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN ns1\").show()\n",
        "\n",
        "print(\"\\nListing tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN ns2\").show()\n",
        "\n",
        "# Insert sample data into tables\n",
        "insert_commands = [\n",
        "    \"\"\"INSERT INTO ns1.table1 (key, value, version) \n",
        "       VALUES \n",
        "       ('key1', 'value1', 1),\n",
        "       ('key2', 'value2', 2),\n",
        "       ('key3', 'value3', 3)\"\"\",\n",
        "    \n",
        "    \"\"\"INSERT INTO ns2.table2 (key, value, version)\n",
        "       VALUES\n",
        "       ('keyA', 'valueA', 10),\n",
        "       ('keyB', 'valueB', 20),\n",
        "       ('keyC', 'valueC', 30)\"\"\"\n",
        "]\n",
        "\n",
        "# Execute inserts\n",
        "for cmd in insert_commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    spark.sql(cmd)\n",
        "    print(\"Success!\")\n",
        "\n",
        "# Verify data in tables\n",
        "print(\"\\nData in ns1.table1:\")\n",
        "spark.sql(\"SELECT * FROM ns1.table1\").show()\n",
        "\n",
        "print(\"\\nData in ns2.table2:\")\n",
        "spark.sql(\"SELECT * FROM ns2.table2\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "polaris-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
