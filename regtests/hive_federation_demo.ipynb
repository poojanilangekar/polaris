{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set JAVA_HOME to: /Users/pnilangekar/.jenv/versions/21\n",
            "SPARK_VERSION set to: 3.5.6\n",
            "Hive distro at /Users/pnilangekar/apache-hive-3.1.3-bin\n",
            "Vefitying Spark conf...\n",
            "# HIVE_METASTORE_ICEBERG_TESTCONF\n",
            "Hive metastore iceberg conf already set\n",
            "Killing any running metastore server...\n",
            "Deleting metastore database and warehouse...\n",
            "Initializing metastore schema...\n",
            "Metastore connection URL:\t jdbc:derby:;databaseName=/tmp/data/hms/metastore_db;;create=true\n",
            "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
            "Metastore connection User:\t APP\n",
            "Starting metastore schema initialization to 3.1.0\n",
            "Initialization script hive-schema-3.1.0.derby.sql\n",
            "Initialization script completed\n",
            "schemaTool completed\n",
            "Starting metastore server...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def run_shell_command(command):\n",
        "    try:\n",
        "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        \n",
        "        # Print output in real-time\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "                \n",
        "        # Get the return code\n",
        "        return_code = process.poll()\n",
        "        \n",
        "        # Print any errors if the command failed\n",
        "        if return_code != 0:\n",
        "            print(\"Error output:\", file=sys.stderr)\n",
        "            print(process.stderr.read(), file=sys.stderr)\n",
        "            raise subprocess.CalledProcessError(return_code, command)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error executing command: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "os.environ[\"JAVA_HOME\"] = \"/Users/pnilangekar/.jenv/versions/21\"\n",
        "print(f\"Set JAVA_HOME to: {os.environ['JAVA_HOME']}\")\n",
        "os.environ['SPARK_VERSION'] = '3.5.6'  # Set Spark version\n",
        "print(f\"SPARK_VERSION set to: {os.environ['SPARK_VERSION']}\")\n",
        "# Execute the setup_hms.sh script with clean_metastore argument\n",
        "run_shell_command(\"./setup_hms.sh clean_metastore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set JAVA_HOME to: /Users/pnilangekar/.jenv/versions/21\n",
            "Spark home:  /Users/pnilangekar/spark-3.5.5-bin-hadoop3\n",
            "JAVA_HOME:  /Users/pnilangekar/.jenv/versions/21\n",
            ":: loading settings :: url = jar:file:/Users/pnilangekar/spark-3.5.5-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /Users/pnilangekar/.ivy2/cache\n",
            "The jars for the packages stored in: /Users/pnilangekar/.ivy2/jars\n",
            "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e91d993f-d90e-46d6-9069-ab828ec7d66c;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 in central\n",
            ":: resolution report :: resolve 52ms :: artifacts dl 1ms\n",
            "\t:: modules in use:\n",
            "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-e91d993f-d90e-46d6-9069-ab828ec7d66c\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n",
            "25/08/14 17:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Session created successfully with Spark version: 3.5.5\n",
            "Spark Configuration: [('spark.files', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.variable.substitute', 'true'), ('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalog.hadoop_cat.warehouse', 'file:///tmp/warehouse'), ('spark.sql.defaultCatalog', 'spark_catalog'), ('spark.app.id', 'local-1755217205750'), ('spark.sql.warehouse.dir', '/tmp/data/spark-warehouse'), ('spark.serializer.objectStreamReset', '100'), ('spark.app.name', 'Hive Federation Demo'), ('spark.master', 'local[*]'), ('spark.sql.catalog.spark_catalog.uri', 'thrift://localhost:9083'), ('spark.submit.deployMode', 'client'), ('spark.app.initial.jar.urls', 'spark://127.0.0.1:61451/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.app.initial.file.urls', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', '127.0.0.1'), ('spark.app.submitTime', '1755217205273'), ('spark.sql.catalog.hadoop_cat.type', 'hadoop'), ('spark.submit.pyFiles', '/Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0'), ('spark.driver.port', '61451'), ('spark.executor.id', 'driver'), ('spark.app.startTime', '1755217205403'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.repl.local.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.catalog.spark_catalog.warehouse', '/tmp/data/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.hadoop_cat', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.catalog.spark_catalog.type', 'hive')]\n",
            "\n",
            "Executing: CREATE NAMESPACE IF NOT EXISTS ns1\n",
            "Success!\n",
            "\n",
            "Executing: CREATE NAMESPACE IF NOT EXISTS ns2\n",
            "Success!\n",
            "\n",
            "Executing: DROP TABLE IF EXISTS ns1.table1\n",
            "Success!\n",
            "\n",
            "Executing: DROP TABLE IF EXISTS ns2.table2\n",
            "Success!\n",
            "\n",
            "Executing: CREATE TABLE ns1.table1 (key STRING, value STRING, version INT) STORED AS ICEBERG\n",
            "Success!\n",
            "\n",
            "Executing: CREATE TABLE ns2.table2 (key STRING, value STRING, version INT) STORED AS ICEBERG\n",
            "Success!\n",
            "\n",
            "--- Inserting Data ---\n",
            "Data inserted successfully.\n",
            "\n",
            "--- Verifying Tables ---\n",
            "Listing all tables in ns1:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns1|   table1|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Listing all tables in ns2:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns2|   table2|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "--- Reading Data ---\n",
            "Data from ns1.table1:\n",
            "+-------+------------+-------+\n",
            "|    key|       value|version|\n",
            "+-------+------------+-------+\n",
            "| Engine|       Spark|      1|\n",
            "| Engine|Apache Spark|      2|\n",
            "|Catalog|        Hive|      1|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Data from ns2.table2:\n",
            "+----------+------------+-------+\n",
            "|       key|       value|version|\n",
            "+----------+------------+-------+\n",
            "|    Engine|   Snowflake|      1|\n",
            "|Alt Engine|Apache Spark|      1|\n",
            "|   Catalog|Open Catalog|      1|\n",
            "+----------+------------+-------+\n",
            "\n",
            "\n",
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# Set JAVA_HOME directly (same value that works in your shell)\n",
        "os.environ[\"JAVA_HOME\"] = \"/Users/pnilangekar/.jenv/versions/21\"\n",
        "print(f\"Set JAVA_HOME to: {os.environ['JAVA_HOME']}\")\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] =\"/Users/pnilangekar/spark-3.5.5-bin-hadoop3\"\n",
        "print(\"Spark home: \", os.environ.get(\"SPARK_HOME\"))\n",
        "print(\"JAVA_HOME: \", os.environ.get(\"JAVA_HOME\"))\n",
        "\n",
        "\n",
        "iceberg_version = \"1.9.0\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# --- Spark Session Builder ---\n",
        "# This configuration is cleaner and follows best practices.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive Federation Demo\") \\\n",
        "    .config(\"spark.jars.packages\", iceberg_package) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/data/spark-warehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Verification ---\n",
        "# Print the Spark version and some key configurations to verify.\n",
        "print(f\"Spark Session created successfully with Spark version: {spark.version}\")\n",
        "conf = spark.sparkContext.getConf()\n",
        "print(f\"Spark Configuration: {conf.getAll()}\")\n",
        "\n",
        "\n",
        "# --- Example Usage: Create Namespaces and Tables ---\n",
        "# The key fix is adding \"USING iceberg\" to your CREATE TABLE statements.\n",
        "# Create namespaces and tables\n",
        "commands = [\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns1\",\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns2\",\n",
        "    \"DROP TABLE IF EXISTS ns1.table1\",\n",
        "    \"DROP TABLE IF EXISTS ns2.table2\",\n",
        "    \"CREATE TABLE ns1.table1 (key STRING, value STRING, version INT) STORED AS ICEBERG\",\n",
        "    \"CREATE TABLE ns2.table2 (key STRING, value STRING, version INT) STORED AS ICEBERG\"\n",
        "]\n",
        "\n",
        "# Execute each command and print the result\n",
        "for cmd in commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    try:\n",
        "        result = spark.sql(cmd)\n",
        "        if cmd.strip().upper().startswith((\"SHOW\", \"SELECT\", \"DESCRIBE\")):\n",
        "            result.show()\n",
        "        else:\n",
        "            print(\"Success!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Insert some data\n",
        "print(\"\\n--- Inserting Data ---\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns1.table1 VALUES ('Engine', 'Spark', 1), ('Engine', 'Apache Spark', 2), ('Catalog', 'Hive', 1)\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns2.table2 VALUES ('Engine', 'Snowflake', 1), ('Alt Engine', 'Apache Spark', 1), ('Catalog', 'Open Catalog', 1)\")\n",
        "print(\"Data inserted successfully.\")\n",
        "\n",
        "# Verify the tables were created\n",
        "print(\"\\n--- Verifying Tables ---\")\n",
        "print(\"Listing all tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns1\").show()\n",
        "print(\"\\nListing all tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns2\").show()\n",
        "\n",
        "# Verify data can be read\n",
        "print(\"\\n--- Reading Data ---\")\n",
        "print(\"Data from ns1.table1:\")\n",
        "spark.table(\"spark_catalog.ns1.table1\").show()\n",
        "print(\"\\nData from ns2.table2:\")\n",
        "spark.table(\"spark_catalog.ns2.table2\").show()\n",
        "\n",
        "\n",
        "# Stop the session when done\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up POLARIS auth token. \n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "POLARIS_HOST = os.getenv('POLARIS_HOST', 'localhost')\n",
        "BASE_URL = f\"http://{POLARIS_HOST}:8181\"\n",
        "\n",
        "def get_polaris_token():\n",
        "    response = requests.post(f\"{BASE_URL}/api/catalog/v1/oauth/tokens\",\n",
        "                             data={'grant_type': 'client_credentials', 'scope': 'PRINCIPAL_ROLE:ALL'},\n",
        "                             auth=('root', 's3cr3t'))\n",
        "    return response.json()['access_token']\n",
        "\n",
        "POLARIS_TOKEN = get_polaris_token()\n",
        "\n",
        "os.environ['POLARIS_TOKEN'] = POLARIS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def curl_command(cmd):\n",
        "    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    print(f\"Status Code: {process.returncode}\")\n",
        "    print(process.stdout if process.stdout else process.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   473    0     0  100   473      0   4106 --:--:-- --:--:-- --:--:--  4113\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the JSON payload\n",
        "json_data = '''{\n",
        "  \"type\": \"EXTERNAL\",\n",
        "  \"name\": \"federated_hive\",\n",
        "  \"connectionConfigInfo\": {\n",
        "    \"connectionType\": \"HIVE\",\n",
        "    \"uri\": \"thrift://127.0.0.1:9083\",\n",
        "    \"warehouse\": \"hms\",\n",
        "    \"authenticationParameters\": {\n",
        "      \"authenticationType\": \"IMPLICIT\"\n",
        "    }\n",
        "  },\n",
        "  \"properties\": {\n",
        "    \"default-base-location\": \"file:///tmp/data/spark-warehouse\"\n",
        "  },\n",
        "  \"storageConfigInfo\": {\n",
        "    \"storageType\": \"FILE\",\n",
        "    \"allowedLocations\": [\n",
        "      \"file:///tmp/data/spark-warehouse\"\n",
        "    ]\n",
        "  }\n",
        "}'''\n",
        "\n",
        "# Construct and execute the curl command\n",
        "cmd = f'''curl -X POST http://polaris:8181/api/management/v1/catalogs \\\n",
        "    --resolve polaris:8181:127.0.0.1 \\\n",
        "    -H \"Accepts: application/json\" \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $POLARIS_TOKEN\" \\\n",
        "    -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 462\n",
            "\n",
            "{\"connectionConfigInfo\":{\"warehouse\":\"hms\",\"connectionType\":\"HIVE\",\"uri\":\"thrift://127.0.0.1:9083\",\"authenticationParameters\":{\"authenticationType\":\"IMPLICIT\"}},\"type\":\"EXTERNAL\",\"name\":\"federated_hive\",\"properties\":{\"default-base-location\":\"file:///tmp/data/spark-warehouse\"},\"createTimestamp\":1755217258155,\"lastUpdateTimestamp\":1755217258155,\"entityVersion\":1,\"storageConfigInfo\":{\"storageType\":\"FILE\",\"allowedLocations\":[\"file:///tmp/data/spark-warehouse\"]}}\n"
          ]
        }
      ],
      "source": [
        "cmd = f'''curl -i -X GET http://localhost:8181/api/management/v1/catalogs/federated_hive/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 78\n",
            "\n",
            "{\"identifiers\":[{\"namespace\":[\"ns1\"],\"name\":\"table1\"}],\"next-page-token\":null}\n"
          ]
        }
      ],
      "source": [
        "# List all tables in the ns1 namespace. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 67\n",
            "\n",
            "{\"namespaces\":[[\"default\"],[\"ns1\"],[\"ns2\"]],\"next-page-token\":null}\n"
          ]
        }
      ],
      "source": [
        "# List all namespaces. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 78\n",
            "\n",
            "{\"identifiers\":[{\"namespace\":[\"ns1\"],\"name\":\"table1\"}],\"next-page-token\":null}\n"
          ]
        }
      ],
      "source": [
        "# List all tables in the ns1 namespace. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/json;charset=UTF-8\n",
            "content-length: 2022\n",
            "ETag: W/\"2dcfc0ca37b4374c31d5ec504a09f0965c0227ed84aa7f33433684a301763ffd\"\n",
            "\n",
            "{\"metadata-location\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/00001-11ea7768-348d-449e-a316-f5fff8cc725e.metadata.json\",\"metadata\":{\"format-version\":2,\"table-uuid\":\"6a6e2a44-2fc2-4e9a-a593-f65f7125e4fa\",\"location\":\"file:/tmp/data/spark-warehouse/ns1.db/table1\",\"last-sequence-number\":1,\"last-updated-ms\":1755217209166,\"last-column-id\":3,\"current-schema-id\":0,\"schemas\":[{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"key\",\"required\":false,\"type\":\"string\"},{\"id\":2,\"name\":\"value\",\"required\":false,\"type\":\"string\"},{\"id\":3,\"name\":\"version\",\"required\":false,\"type\":\"int\"}]}],\"default-spec-id\":0,\"partition-specs\":[{\"spec-id\":0,\"fields\":[]}],\"last-partition-id\":999,\"default-sort-order-id\":0,\"sort-orders\":[{\"order-id\":0,\"fields\":[]}],\"properties\":{\"owner\":\"pnilangekar\",\"hive.stored-as\":\"ICEBERG\",\"write.parquet.compression-codec\":\"zstd\"},\"current-snapshot-id\":8535457035882100069,\"refs\":{\"main\":{\"snapshot-id\":8535457035882100069,\"type\":\"branch\"}},\"snapshots\":[{\"sequence-number\":1,\"snapshot-id\":8535457035882100069,\"timestamp-ms\":1755217209166,\"summary\":{\"operation\":\"append\",\"spark.app.id\":\"local-1755217205750\",\"added-data-files\":\"3\",\"added-records\":\"3\",\"added-files-size\":\"2877\",\"changed-partition-count\":\"1\",\"total-records\":\"3\",\"total-files-size\":\"2877\",\"total-data-files\":\"3\",\"total-delete-files\":\"0\",\"total-position-deletes\":\"0\",\"total-equality-deletes\":\"0\",\"engine-version\":\"3.5.5\",\"app-id\":\"local-1755217205750\",\"engine-name\":\"spark\",\"iceberg-version\":\"Apache Iceberg unspecified (commit 7dbafb438ee1e68d0047bebcb587265d7d87d8a1)\"},\"manifest-list\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/snap-8535457035882100069-1-20649834-de00-4015-a8a9-111cc6a420db.avro\",\"schema-id\":0}],\"statistics\":[],\"partition-statistics\":[],\"snapshot-log\":[{\"timestamp-ms\":1755217209166,\"snapshot-id\":8535457035882100069}],\"metadata-log\":[{\"timestamp-ms\":1755217207439,\"metadata-file\":\"file:/tmp/data/spark-warehouse/ns1.db/table1/metadata/00000-6a4a834b-e116-4078-9678-727969401389.metadata.json\"}]}}\n"
          ]
        }
      ],
      "source": [
        "# List table metadata for ns1.table1. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/table1 \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 201 Created\n",
            "content-encoding: identity\n",
            "content-length: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Grant TABLE_WRITE_DATA to the catalog_admin role. \n",
        "json_data = '''{\"type\": \"catalog\", \"privilege\": \"TABLE_WRITE_DATA\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/catalogs/federated_hive/catalog-roles/catalog_admin/grants \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 0\n",
            "HTTP/1.1 201 Created\n",
            "content-encoding: identity\n",
            "content-length: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assign the catalog_admin role to the service_admin.\n",
        "json_data = '''{\"name\": \"catalog_admin\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/principal-roles/service_admin/catalog-roles/federated_hive \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 3.5.5\n",
            "Active Spark Configuration:\n",
            "[('spark.files', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.variable.substitute', 'true'), ('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalog.hadoop_cat.warehouse', 'file:///tmp/warehouse'), ('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation', 'vended-credentials'), ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.iceberg:iceberg-aws-bundle:1.7.1,software.amazon.awssdk:bundle:2.30.25,net.snowflake:snowflake-jdbc:3.13.32,com.azure:azure-storage-file-datalake:12.17.0,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.5'), ('spark.sql.catalog.polaris.type', 'rest'), ('spark.sql.catalog.polaris.token', 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwb2xhcmlzIiwic3ViIjoiMSIsImlhdCI6MTc1NTIxNzI1MywiZXhwIjoxNzU1MjIwODUzLCJqdGkiOiI2N2U2MGRmMC02OGNmLTQyNTQtOGUxMS0yNTg5ODI4NzQxZDYiLCJhY3RpdmUiOnRydWUsImNsaWVudF9pZCI6InJvb3QiLCJwcmluY2lwYWxJZCI6MSwic2NvcGUiOiJQUklOQ0lQQUxfUk9MRTpBTEwifQ.F6Wx5JWec0d_M1eg84dxNhhq9ScVqY_Qkj3Hi-s6wzf8GuZxqFhUq5dqtrLU5SyGo4jTVXZn3muUIs17uGwbOMFP2KZ8dZUaDKhQNQ1F_nBojcNxsZ6WPjycTJr5kPC4wbxoYOcpKhX7DVDUnCQzd7VeEIqip_Li0yyzcI1j1aVwQAqI4cTG0LVkOx_V0XSAHIj1m1SmtxW69LUm0aKyympAGM9_t4lIxNltR3rD1iosEVsUyjUu658xZsZlfQX5iH6t1pNuZjqF3SpcBl7ejXAKjmMNjGDw3_EDoTxIOZcfTfQTqjlANwbnOap19XpS4oOowjAe2tkec6v1f5uOYg'), ('spark.sql.warehouse.dir', '/tmp/data/spark-warehouse'), ('spark.app.startTime', '1755217275757'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalog.spark_catalog.uri', 'thrift://localhost:9083'), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'Polaris Hive Federation'), ('spark.app.initial.jar.urls', 'spark://127.0.0.1:64364/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.app.initial.file.urls', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', '127.0.0.1'), ('spark.sql.catalog.polaris.client.region', 'us-east-1'), ('spark.app.submitTime', '1755217205273'), ('spark.sql.catalog.hadoop_cat.type', 'hadoop'), ('spark.submit.pyFiles', '/Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.defaultCatalog', 'polaris'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1755217275848'), ('spark.sql.catalog.polaris.uri', 'http://localhost:8181/api/catalog'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.polaris.warehouse', 'federated_hive'), ('spark.driver.port', '64364'), ('spark.repl.local.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.catalog.spark_catalog.warehouse', '/tmp/data/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.hadoop_cat', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.catalog.spark_catalog.type', 'hive')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "iceberg_version = \"1.9.0\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# Create Spark session with the same configurations as spark-sql command\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Polaris Hive Federation\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.jars.packages\",  \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.iceberg:iceberg-aws-bundle:1.7.1,software.amazon.awssdk:bundle:2.30.25,net.snowflake:snowflake-jdbc:3.13.32,com.azure:azure-storage-file-datalake:12.17.0,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.5\") \\\n",
        "     .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.token\", os.environ.get('POLARIS_TOKEN')) \\\n",
        "    .config(\"spark.sql.catalog.polaris.type\", \"rest\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.uri\", \"http://localhost:8181/api/catalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.warehouse\", \"federated_hive\") \\\n",
        "    .config(\"spark.sql.defaultCatalog\", \"polaris\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.client.region\", \"us-east-1\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", \"vended-credentials\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "# Verify configuration\n",
        "print(\"Active Spark Configuration:\")\n",
        "print(spark.sparkContext.getConf().getAll())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing all namespaces:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/14 17:21:19 WARN AuthManagers: Inferring rest.auth.type=oauth2 since property token was provided. Please explicitly set rest.auth.type to avoid this warning.\n",
            "25/08/14 17:21:19 WARN OAuth2Manager: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://localhost:8181/api/catalog/v1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|      ns1|\n",
            "|      ns2|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Listing tables in ns1:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns1|   table1|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Listing tables in ns2:\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|      ns2|   table2|      false|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "\n",
            "Executing: INSERT INTO ns1.table1 (key, value, version) \n",
            "       VALUES \n",
            "       ('key1', 'value1', 1),\n",
            "       ('key2', 'value2', 2),\n",
            "       ('key3', 'value3', 3)\n",
            "Success!\n",
            "\n",
            "Executing: INSERT INTO ns2.table2 (key, value, version)\n",
            "       VALUES\n",
            "       ('keyA', 'valueA', 10),\n",
            "       ('keyB', 'valueB', 20),\n",
            "       ('keyC', 'valueC', 30)\n",
            "Success!\n",
            "\n",
            "Data in ns1.table1:\n",
            "+-------+------------+-------+\n",
            "|    key|       value|version|\n",
            "+-------+------------+-------+\n",
            "|   key1|      value1|      1|\n",
            "|   key2|      value2|      2|\n",
            "|   key3|      value3|      3|\n",
            "| Engine|       Spark|      1|\n",
            "| Engine|Apache Spark|      2|\n",
            "|Catalog|        Hive|      1|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Data in ns2.table2:\n",
            "+----------+------------+-------+\n",
            "|       key|       value|version|\n",
            "+----------+------------+-------+\n",
            "|      keyA|      valueA|     10|\n",
            "|      keyB|      valueB|     20|\n",
            "|      keyC|      valueC|     30|\n",
            "|    Engine|   Snowflake|      1|\n",
            "|Alt Engine|Apache Spark|      1|\n",
            "|   Catalog|Open Catalog|      1|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/14 17:21:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "# List all namespaces\n",
        "print(\"Listing all namespaces:\")\n",
        "spark.sql(\"SHOW NAMESPACES\").show()\n",
        "\n",
        "# List tables in ns1 and ns2\n",
        "print(\"\\nListing tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN ns1\").show()\n",
        "\n",
        "print(\"\\nListing tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN ns2\").show()\n",
        "\n",
        "# Insert sample data into tables\n",
        "insert_commands = [\n",
        "    \"\"\"INSERT INTO ns1.table1 (key, value, version) \n",
        "       VALUES \n",
        "       ('key1', 'value1', 1),\n",
        "       ('key2', 'value2', 2),\n",
        "       ('key3', 'value3', 3)\"\"\",\n",
        "    \n",
        "    \"\"\"INSERT INTO ns2.table2 (key, value, version)\n",
        "       VALUES\n",
        "       ('keyA', 'valueA', 10),\n",
        "       ('keyB', 'valueB', 20),\n",
        "       ('keyC', 'valueC', 30)\"\"\"\n",
        "]\n",
        "\n",
        "# Execute inserts\n",
        "for cmd in insert_commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    spark.sql(cmd)\n",
        "    print(\"Success!\")\n",
        "\n",
        "# Verify data in tables\n",
        "print(\"\\nData in ns1.table1:\")\n",
        "spark.sql(\"SELECT * FROM ns1.table1\").show()\n",
        "\n",
        "print(\"\\nData in ns2.table2:\")\n",
        "spark.sql(\"SELECT * FROM ns2.table2\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/14 17:21:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Session created successfully with Spark version: 3.5.5\n",
            "Spark Configuration: [('spark.files', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.variable.substitute', 'true'), ('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalog.hadoop_cat.warehouse', 'file:///tmp/warehouse'), ('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation', 'vended-credentials'), ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.iceberg:iceberg-aws-bundle:1.7.1,software.amazon.awssdk:bundle:2.30.25,net.snowflake:snowflake-jdbc:3.13.32,com.azure:azure-storage-file-datalake:12.17.0,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.5'), ('spark.sql.catalog.polaris.type', 'rest'), ('spark.sql.catalog.polaris.token', 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwb2xhcmlzIiwic3ViIjoiMSIsImlhdCI6MTc1NTIxNzI1MywiZXhwIjoxNzU1MjIwODUzLCJqdGkiOiI2N2U2MGRmMC02OGNmLTQyNTQtOGUxMS0yNTg5ODI4NzQxZDYiLCJhY3RpdmUiOnRydWUsImNsaWVudF9pZCI6InJvb3QiLCJwcmluY2lwYWxJZCI6MSwic2NvcGUiOiJQUklOQ0lQQUxfUk9MRTpBTEwifQ.F6Wx5JWec0d_M1eg84dxNhhq9ScVqY_Qkj3Hi-s6wzf8GuZxqFhUq5dqtrLU5SyGo4jTVXZn3muUIs17uGwbOMFP2KZ8dZUaDKhQNQ1F_nBojcNxsZ6WPjycTJr5kPC4wbxoYOcpKhX7DVDUnCQzd7VeEIqip_Li0yyzcI1j1aVwQAqI4cTG0LVkOx_V0XSAHIj1m1SmtxW69LUm0aKyympAGM9_t4lIxNltR3rD1iosEVsUyjUu658xZsZlfQX5iH6t1pNuZjqF3SpcBl7ejXAKjmMNjGDw3_EDoTxIOZcfTfQTqjlANwbnOap19XpS4oOowjAe2tkec6v1f5uOYg'), ('spark.sql.warehouse.dir', 'file:/tmp/data/spark-warehouse'), ('spark.app.startTime', '1755217275757'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.catalog.spark_catalog.uri', 'thrift://localhost:9083'), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'Polaris Hive Federation'), ('spark.app.initial.jar.urls', 'spark://127.0.0.1:64364/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.app.initial.file.urls', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', '127.0.0.1'), ('spark.sql.catalog.polaris.client.region', 'us-east-1'), ('spark.app.submitTime', '1755217205273'), ('spark.sql.catalog.hadoop_cat.type', 'hadoop'), ('spark.submit.pyFiles', '/Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.defaultCatalog', 'polaris'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1755217275848'), ('spark.sql.catalog.polaris.uri', 'http://localhost:8181/api/catalog'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.polaris.warehouse', 'federated_hive'), ('spark.driver.port', '64364'), ('spark.repl.local.jars', 'file:///Users/pnilangekar/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.0.jar'), ('spark.sql.catalog.spark_catalog.warehouse', '/tmp/data/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.hadoop_cat', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.catalog.spark_catalog.type', 'hive')]\n",
            "\n",
            "Data in ns1.table1:\n",
            "+-------+------------+-------+\n",
            "|    key|       value|version|\n",
            "+-------+------------+-------+\n",
            "|   key1|      value1|      1|\n",
            "| Engine|       Spark|      1|\n",
            "|   key2|      value2|      2|\n",
            "| Engine|Apache Spark|      2|\n",
            "|   key3|      value3|      3|\n",
            "|Catalog|        Hive|      1|\n",
            "+-------+------------+-------+\n",
            "\n",
            "\n",
            "Data in ns2.table2:\n",
            "+----------+------------+-------+\n",
            "|       key|       value|version|\n",
            "+----------+------------+-------+\n",
            "|    Engine|   Snowflake|      1|\n",
            "|      keyA|      valueA|     10|\n",
            "|Alt Engine|Apache Spark|      1|\n",
            "|      keyB|      valueB|     20|\n",
            "|   Catalog|Open Catalog|      1|\n",
            "|      keyC|      valueC|     30|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive Federation Demo\") \\\n",
        "    .config(\"spark.jars.packages\", iceberg_package) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/data/spark-warehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Verification ---\n",
        "# Print the Spark version and some key configurations to verify.\n",
        "print(f\"Spark Session created successfully with Spark version: {spark.version}\")\n",
        "conf = spark.sparkContext.getConf()\n",
        "print(f\"Spark Configuration: {conf.getAll()}\")\n",
        "\n",
        "# Verify data in tables\n",
        "print(\"\\nData in ns1.table1:\")\n",
        "spark.sql(\"SELECT * FROM ns1.table1\").show()\n",
        "\n",
        "print(\"\\nData in ns2.table2:\")\n",
        "spark.sql(\"SELECT * FROM ns2.table2\").show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "polaris-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
