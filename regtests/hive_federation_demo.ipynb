{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def run_shell_command(command):\n",
        "    try:\n",
        "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        \n",
        "        # Print output in real-time\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "                \n",
        "        # Get the return code\n",
        "        return_code = process.poll()\n",
        "        \n",
        "        # Print any errors if the command failed\n",
        "        if return_code != 0:\n",
        "            print(\"Error output:\", file=sys.stderr)\n",
        "            print(process.stderr.read(), file=sys.stderr)\n",
        "            raise subprocess.CalledProcessError(return_code, command)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error executing command: {e}\", file=sys.stderr)\n",
        "        raise\n",
        "\n",
        "os.environ['SPARK_VERSION'] = '3.5.5'  # Set Spark version\n",
        "print(f\"SPARK_VERSION set to: {os.environ['SPARK_VERSION']}\")\n",
        "# Execute the setup_hms.sh script with clean_metastore argument\n",
        "run_shell_command(\"./setup_hms.sh clean_metastore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "iceberg_version = \"1.9.0\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# --- Spark Session Builder ---\n",
        "# This configuration is cleaner and follows best practices.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Hive Federation Demo\") \\\n",
        "    .config(\"spark.jars.packages\", iceberg_package) \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/data/spark-warehouse\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Verification ---\n",
        "# Print the Spark version and some key configurations to verify.\n",
        "print(f\"Spark Session created successfully with Spark version: {spark.version}\")\n",
        "conf = spark.sparkContext.getConf()\n",
        "print(f\"Spark Configuration: {conf.getAll()}\")\n",
        "\n",
        "\n",
        "# --- Example Usage: Create Namespaces and Tables ---\n",
        "# The key fix is adding \"USING iceberg\" to your CREATE TABLE statements.\n",
        "# Create namespaces and tables\n",
        "commands = [\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns1\",\n",
        "    \"CREATE NAMESPACE IF NOT EXISTS ns2\",\n",
        "    \"DROP TABLE IF EXISTS ns1.table1\",\n",
        "    \"DROP TABLE IF EXISTS ns2.table2\",\n",
        "    \"CREATE TABLE ns1.table1 (key STRING, value STRING, version INT) STORED AS ICEBERG\",\n",
        "    \"CREATE TABLE ns2.table2 (key STRING, value STRING, version INT) STORED AS ICEBERG\"\n",
        "]\n",
        "\n",
        "# Execute each command and print the result\n",
        "for cmd in commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    try:\n",
        "        result = spark.sql(cmd)\n",
        "        if cmd.strip().upper().startswith((\"SHOW\", \"SELECT\", \"DESCRIBE\")):\n",
        "            result.show()\n",
        "        else:\n",
        "            print(\"Success!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Insert some data\n",
        "print(\"\\n--- Inserting Data ---\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns1.table1 VALUES ('Engine', 'Spark', 1), ('Engine', 'Apache Spark', 2), ('Catalog', 'Hive', 1)\")\n",
        "spark.sql(\"INSERT INTO spark_catalog.ns2.table2 VALUES ('Engine', 'Snowflake', 1), ('Alt Engine', 'Apache Spark', 1), ('Catalog', 'Open Catalog', 1)\")\n",
        "print(\"Data inserted successfully.\")\n",
        "\n",
        "# Verify the tables were created\n",
        "print(\"\\n--- Verifying Tables ---\")\n",
        "print(\"Listing all tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns1\").show()\n",
        "print(\"\\nListing all tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN spark_catalog.ns2\").show()\n",
        "\n",
        "# Verify data can be read\n",
        "print(\"\\n--- Reading Data ---\")\n",
        "print(\"Data from ns1.table1:\")\n",
        "spark.table(\"spark_catalog.ns1.table1\").show()\n",
        "print(\"\\nData from ns2.table2:\")\n",
        "spark.table(\"spark_catalog.ns2.table2\").show()\n",
        "\n",
        "\n",
        "# Stop the session when done\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up POLARIS auth token. \n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "POLARIS_HOST = os.getenv('POLARIS_HOST', 'localhost')\n",
        "BASE_URL = f\"http://{POLARIS_HOST}:8181\"\n",
        "\n",
        "def get_polaris_token():\n",
        "    response = requests.post(f\"{BASE_URL}/api/catalog/v1/oauth/tokens\",\n",
        "                             data={'grant_type': 'client_credentials', 'scope': 'PRINCIPAL_ROLE:ALL'},\n",
        "                             auth=('root', 's3cr3t'))\n",
        "    return response.json()['access_token']\n",
        "\n",
        "POLARIS_TOKEN = get_polaris_token()\n",
        "\n",
        "os.environ['POLARIS_TOKEN'] = POLARIS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def curl_command(cmd):\n",
        "    process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    print(f\"Status Code: {process.returncode}\")\n",
        "    print(process.stdout if process.stdout else process.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the JSON payload\n",
        "json_data = '''{\n",
        "  \"type\": \"EXTERNAL\",\n",
        "  \"name\": \"federated_hive\",\n",
        "  \"connectionConfigInfo\": {\n",
        "    \"connectionType\": \"HIVE\",\n",
        "    \"uri\": \"thrift://127.0.0.1:9083\",\n",
        "    \"warehouse\": \"hms\",\n",
        "    \"authenticationParameters\": {\n",
        "      \"authenticationType\": \"IMPLICIT\"\n",
        "    }\n",
        "  },\n",
        "  \"properties\": {\n",
        "    \"default-base-location\": \"file:///tmp/data/spark-warehouse\"\n",
        "  },\n",
        "  \"storageConfigInfo\": {\n",
        "    \"storageType\": \"FILE\",\n",
        "    \"allowedLocations\": [\n",
        "      \"file:///tmp/data/spark-warehouse\"\n",
        "    ]\n",
        "  }\n",
        "}'''\n",
        "\n",
        "# Construct and execute the curl command\n",
        "cmd = f'''curl -X POST http://polaris:8181/api/management/v1/catalogs \\\n",
        "    --resolve polaris:8181:127.0.0.1 \\\n",
        "    -H \"Accepts: application/json\" \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $POLARIS_TOKEN\" \\\n",
        "    -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = f'''curl -i -X GET http://localhost:8181/api/management/v1/catalogs/federated_hive/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all tables in the ns1 namespace. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all namespaces. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all tables in the ns1 namespace. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/ \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List table metadata for ns1.table1. \n",
        "cmd = f'''curl -i -X GET http://localhost:8181/api/catalog/v1/federated_hive/namespaces/ns1/tables/table1 \\\n",
        "  -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grant TABLE_WRITE_DATA to the catalog_admin role. \n",
        "json_data = '''{\"type\": \"catalog\", \"privilege\": \"TABLE_WRITE_DATA\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/catalogs/federated_hive/catalog-roles/catalog_admin/grants \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign the catalog_admin role to the service_admin.\n",
        "json_data = '''{\"name\": \"catalog_admin\"}'''\n",
        "cmd = f'''curl -i -X PUT -H \"Authorization: Bearer $POLARIS_TOKEN\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" \\\n",
        "    http://localhost:8181/api/management/v1/principal-roles/service_admin/catalog-roles/federated_hive \\\n",
        "  -d '{json_data}' '''\n",
        "\n",
        "curl_command(cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "iceberg_version = \"1.9.0\"\n",
        "spark_version = \"3.5\"\n",
        "scala_version = \"2.12\"\n",
        "iceberg_package = f\"org.apache.iceberg:iceberg-spark-runtime-{spark_version}_{scala_version}:{iceberg_version}\"\n",
        "\n",
        "# Create Spark session with the same configurations as spark-sql command\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Polaris Hive Federation\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.jars.packages\",  \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.iceberg:iceberg-aws-bundle:1.7.1,software.amazon.awssdk:bundle:2.30.25,net.snowflake:snowflake-jdbc:3.13.32,com.azure:azure-storage-file-datalake:12.17.0,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.5\") \\\n",
        "     .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.token\", os.environ.get('POLARIS_TOKEN')) \\\n",
        "    .config(\"spark.sql.catalog.polaris.type\", \"rest\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.uri\", \"http://localhost:8181/api/catalog\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.warehouse\", \"federated_hive\") \\\n",
        "    .config(\"spark.sql.defaultCatalog\", \"polaris\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.client.region\", \"us-east-1\") \\\n",
        "    .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", \"vended-credentials\") \\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "# Verify configuration\n",
        "print(\"Active Spark Configuration:\")\n",
        "print(spark.sparkContext.getConf().getAll())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all namespaces\n",
        "print(\"Listing all namespaces:\")\n",
        "spark.sql(\"SHOW NAMESPACES\").show()\n",
        "\n",
        "# List tables in ns1 and ns2\n",
        "print(\"\\nListing tables in ns1:\")\n",
        "spark.sql(\"SHOW TABLES IN ns1\").show()\n",
        "\n",
        "print(\"\\nListing tables in ns2:\")\n",
        "spark.sql(\"SHOW TABLES IN ns2\").show()\n",
        "\n",
        "# Insert sample data into tables\n",
        "insert_commands = [\n",
        "    \"\"\"INSERT INTO ns1.table1 (key, value, version) \n",
        "       VALUES \n",
        "       ('key1', 'value1', 1),\n",
        "       ('key2', 'value2', 2),\n",
        "       ('key3', 'value3', 3)\"\"\",\n",
        "    \n",
        "    \"\"\"INSERT INTO ns2.table2 (key, value, version)\n",
        "       VALUES\n",
        "       ('keyA', 'valueA', 10),\n",
        "       ('keyB', 'valueB', 20),\n",
        "       ('keyC', 'valueC', 30)\"\"\"\n",
        "]\n",
        "\n",
        "# Execute inserts\n",
        "for cmd in insert_commands:\n",
        "    print(f\"\\nExecuting: {cmd}\")\n",
        "    spark.sql(cmd)\n",
        "    print(\"Success!\")\n",
        "\n",
        "# Verify data in tables\n",
        "print(\"\\nData in ns1.table1:\")\n",
        "spark.sql(\"SELECT * FROM ns1.table1\").show()\n",
        "\n",
        "print(\"\\nData in ns2.table2:\")\n",
        "spark.sql(\"SELECT * FROM ns2.table2\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "polaris-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
